# inference_with_victoriametrics.py
# ä»Victoriametricsè·å–CPUä½¿ç”¨ç‡æ•°æ®å¹¶è¿›è¡Œå¼‚å¸¸æ£€æµ‹
import os
import joblib
import pandas as pd
import numpy as np
from typing import Tuple, List, Optional, Dict, Any
from datetime import datetime, timedelta
import warnings
import requests
import json
from urllib.parse import urlencode

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore", category=FutureWarning, module="pandas")
warnings.filterwarnings("ignore", message="X does not have valid feature names")
warnings.filterwarnings("ignore", message="DataFrame.fillna with 'method' is deprecated")

# å¿½ç•¥SSLéªŒè¯è­¦å‘Šï¼ˆä»…ç”¨äºå¼€å‘ç¯å¢ƒï¼‰
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ======================
# ç‰¹å¾å·¥ç¨‹ç±»ï¼ˆä¸ ZenML pipeline ä¸€è‡´ï¼‰
# ======================
class CPUFeatureEngineer:
    """CPU ç‰¹å¾å·¥ç¨‹å¤„ç†å™¨ï¼Œç”¨äºå¼‚å¸¸æ£€æµ‹ï¼ˆä¸è®­ç»ƒ pipeline ä¿æŒä¸€è‡´ï¼‰"""

    def __init__(self, window_sizes: List[int] = None):
        self.window_sizes = window_sizes or [5, 15, 30, 60]  # åˆ†é’Ÿ
        self.feature_names = []

    def create_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºåŸºç¡€ç‰¹å¾"""
        df = df.copy()
        cpu_col = "cpu_utilization"

        # å½’ä¸€åŒ–
        df["cpu_normalized"] = (df[cpu_col] - df[cpu_col].min()) / (df[cpu_col].max() - df[cpu_col].min())

        # æ’åç™¾åˆ†ä½
        df["cpu_rank"] = df[cpu_col].rank(pct=True)

        # å…¨å±€åç¦»åº¦
        mean_cpu = df[cpu_col].mean()
        std_cpu = df[cpu_col].std()
        df["cpu_deviation_from_mean"] = (df[cpu_col] - mean_cpu) / (std_cpu + 1e-8)

        # æŒ‰å°æ—¶ã€æŒ‰æ—¥åŸºçº¿åç¦»
        df["hour"] = df["timestamp"].dt.hour
        df["day_of_week"] = df["timestamp"].dt.dayofweek
        df["date"] = df["timestamp"].dt.date

        hourly_mean = df.groupby("hour")[cpu_col].transform("mean")
        daily_mean = df.groupby("date")[cpu_col].transform("mean")
        df["cpu_deviation_hourly"] = df[cpu_col] - hourly_mean
        df["cpu_deviation_daily"] = df[cpu_col] - daily_mean

        # é˜ˆå€¼æ ‡è®°
        df["is_high_usage"] = (df[cpu_col] > 80).astype(int)
        df["is_very_high_usage"] = (df[cpu_col] > 90).astype(int)
        df["is_low_usage"] = (df[cpu_col] < 10).astype(int)

        self.feature_names.extend([
            "cpu_normalized", "cpu_rank", "cpu_deviation_from_mean",
            "cpu_deviation_hourly", "cpu_deviation_daily",
            "is_high_usage", "is_very_high_usage", "is_low_usage"
        ])
        return df

    def create_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºæ—¶é—´åºåˆ—ç‰¹å¾"""
        df = df.copy()
        cpu_col = "cpu_utilization"

        for window in self.window_sizes:
            win = f"{window}min"
            # å°† window è½¬æ¢ä¸ºåŸºäº 60 ç§’çš„æ•´æ•°ï¼ˆå¦‚ 5min = 5*60 = 300 è¡Œï¼‰
            rolled = df[cpu_col].rolling(window=window * 60, min_periods=1)

            df[f"{win}_mean"] = rolled.mean()
            df[f"{win}_std"] = rolled.std().fillna(0)
            df[f"{win}_max"] = rolled.max()
            df[f"{win}_min"] = rolled.min()
            df[f"{win}_range"] = df[f"{win}_max"] - df[f"{win}_min"]
            df[f"{win}_change"] = df[cpu_col] - df[f"{win}_mean"]
            df[f"{win}_pct_change"] = df[cpu_col] / (df[f"{win}_mean"] + 1e-8) - 1

            # è¶‹åŠ¿ï¼šçº¿æ€§å›å½’æ–œç‡è¿‘ä¼¼
            try:
                df[f"{win}_trend"] = rolled.apply(
                    lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) >= 2 else 0,
                    raw=True
                )
            except:
                df[f"{win}_trend"] = 0

            self.feature_names.extend([
                f"{win}_mean", f"{win}_std", f"{win}_max", f"{win}_min",
                f"{win}_range", f"{win}_change", f"{win}_pct_change", f"{win}_trend"
            ])

        # æ—¶é—´ä¸Šä¸‹æ–‡
        df["hour"] = df["timestamp"].dt.hour
        df["weekday"] = df["timestamp"].dt.dayofweek
        df["minute"] = df["timestamp"].dt.minute
        df["is_weekend"] = (df["weekday"] >= 5).astype(int)
        df["is_business_hour"] = ((df["hour"] >= 9) & (df["hour"] <= 17)).astype(int)
        df["is_peak_hour"] = ((df["hour"] >= 10) & (df["hour"] <= 16)).astype(int)

        self.feature_names.extend(["hour", "weekday", "minute", "is_weekend", "is_business_hour", "is_peak_hour"])
        return df

    def create_algorithm_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºåŸºäºç®—æ³•çš„å¼‚å¸¸æ£€æµ‹ç‰¹å¾"""
        from sklearn.ensemble import IsolationForest
        from sklearn.neighbors import LocalOutlierFactor

        df = df.copy()
        cpu_col = "cpu_utilization"

        # Isolation Forest
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        df["iso_forest_anomaly"] = iso_forest.fit_predict(df[[cpu_col]])
        df["iso_forest_anomaly"] = (df["iso_forest_anomaly"] == -1).astype(int)

        # LOF
        lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)
        df["lof_anomaly"] = lof.fit_predict(df[[cpu_col]])
        df["lof_anomaly"] = (df["lof_anomaly"] == -1).astype(int)

        # Z-score
        df["z_score"] = (df[cpu_col] - df[cpu_col].mean()) / (df[cpu_col].std() + 1e-8)
        df["z_score_anomaly"] = (df["z_score"].abs() > 3).astype(int)

        # IQR
        Q1 = df[cpu_col].quantile(0.25)
        Q3 = df[cpu_col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        df["iqr_anomaly"] = ((df[cpu_col] < lower) | (df[cpu_col] > upper)).astype(int)

        # ç®—æ³•å…±è¯†
        df["algorithm_consensus"] = df[["iso_forest_anomaly", "lof_anomaly", "z_score_anomaly", "iqr_anomaly"]].sum(axis=1)
        df["algorithm_agreement"] = (df["algorithm_consensus"] >= 2).astype(int)

        self.feature_names.extend([
            "iso_forest_anomaly", "lof_anomaly", "z_score", "z_score_anomaly",
            "iqr_anomaly", "algorithm_consensus", "algorithm_agreement"
        ])
        return df

    def create_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºäº¤äº’ä¸å¤åˆç‰¹å¾"""
        df = df.copy()

        # ç®—æ³•ä¸ç»Ÿè®¡äº¤äº’
        df["iso_x_deviation"] = df["iso_forest_anomaly"] * df["cpu_deviation_from_mean"]
        df["lof_x_zscore"] = df["lof_anomaly"] * df["z_score"]

        # æŒç»­æ€§ç‰¹å¾
        for n in [3, 5, 10]:
            df[f"high_usage_streak_{n}"] = (
                df["is_high_usage"].rolling(n, min_periods=1).sum()
            )
            df[f"anomaly_streak_{n}"] = (
                df["algorithm_agreement"].rolling(n, min_periods=1).sum()
            )

        # æ³¢åŠ¨æ€§
        for window in [15, 30, 60]:
            win = f"{window}min"
            df[f"{win}_cv"] = df[f"{win}_std"] / (df[f"{win}_mean"] + 1e-8)  # å˜å¼‚ç³»æ•°

        self.feature_names.extend([
            "iso_x_deviation", "lof_x_zscore",
            "high_usage_streak_3", "high_usage_streak_5", "high_usage_streak_10",
            "anomaly_streak_3", "anomaly_streak_5", "anomaly_streak_10",
            "15min_cv", "30min_cv", "60min_cv"
        ])
        return df

    def create_all_features(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
        """ä¸»æµç¨‹ï¼šä¾æ¬¡æ„å»ºæ‰€æœ‰ç‰¹å¾"""
        start_time = 0  # ä¸è®°å½•æ—¥å¿—

        # ç¡®ä¿ timestamp æ˜¯ datetime ç±»å‹
        df["timestamp"] = pd.to_datetime(df["timestamp"], unit='s')  # âœ… å…³é”®ï¼šUnix æ—¶é—´æˆ³è½¬ datetime
        df = df.sort_values("timestamp").reset_index(drop=True)

        print(f"ğŸ“Š å¼€å§‹ç‰¹å¾å·¥ç¨‹ï¼ŒåŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")

        df = self.create_basic_features(df)
        df = self.create_temporal_features(df)
        df = self.create_algorithm_features(df)
        df = self.create_interaction_features(df)

        # å¤„ç†ç¼ºå¤±å€¼
        df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)

        total_time = 0
        print(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œæœ€ç»ˆæ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"ğŸ§© ç”Ÿæˆç‰¹å¾æ•°é‡: {len(self.feature_names)}")

        return df, self.feature_names


# ======================
# æ•°æ®è·å–ä¸å†™å…¥å‡½æ•°
# ======================
def fetch_cpu_data_from_victoriametrics(
    vm_url: str,
    query: str,
    start_time: str,
    end_time: str,
    step: str = "30",
    token: str = "xxxxxxx",
    namespace: str = "dip-Prod",
    host_name: str = "PDIPMCNINFVM005"
) -> pd.DataFrame:
    """
    ä» Victoriametrics è·å– CPU ä½¿ç”¨ç‡æ•°æ®
    
    Args:
        vm_url: Victoriametrics API URL
        query: æŸ¥è¯¢è¯­å¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æŸ¥è¯¢
        start_time: å¼€å§‹æ—¶é—´ï¼ŒISO æ ¼å¼ (YYYY-MM-DDTHH:MM:SSZ)
        end_time: ç»“æŸæ—¶é—´ï¼ŒISO æ ¼å¼ (YYYY-MM-DDTHH:MM:SSZ)
        step: æ­¥é•¿ï¼Œå•ä½ç§’
        token: è®¤è¯ä»¤ç‰Œ
        namespace: å‘½åç©ºé—´
        host_name: ä¸»æœºå
        
    Returns:
        åŒ…å« timestamp å’Œ cpu_utilization çš„ DataFrame
    """
    if query is None:
        # é»˜è®¤æŸ¥è¯¢
        query = f'(1 - avg by (project, namespace, host_name) (rate(system_cpu_time_seconds_total{{state="idle",namespace="{namespace}",host_name="{host_name}"}}[1m]))) * 100'
    
    # æ„å»ºè¯·æ±‚å‚æ•°
    params = {
        'query': query,
        'start': start_time,
        'end': end_time,
        'step': step
    }
    
    # æ„å»ºè¯·æ±‚å¤´
    headers = {
        'Authorization': f'Bearer {token}'
    }
    
    print(f"ğŸ” æ­£åœ¨ä» Victoriametrics è·å–æ•°æ®...")
    print(f"   æ—¶é—´èŒƒå›´: {start_time} è‡³ {end_time}")
    print(f"   æ­¥é•¿: {step}ç§’")
    
    try:
        # å‘é€è¯·æ±‚
        response = requests.post(
            vm_url,
            data=params,
            headers=headers,
            verify=False  # å¿½ç•¥SSLè¯ä¹¦éªŒè¯
        )
        
        # æ£€æŸ¥å“åº”çŠ¶æ€
        response.raise_for_status()
        
        # è§£æå“åº”
        data = response.json()
        
        if data['status'] != 'success':
            raise ValueError(f"API è¿”å›é”™è¯¯: {data.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        # æå–ç»“æœ
        result = data['data']['result']
        
        if not result:
            raise ValueError("æŸ¥è¯¢ç»“æœä¸ºç©º")
        
        # æå–ç¬¬ä¸€ä¸ªç»“æœçš„å€¼
        values = result[0]['values']
        
        # è½¬æ¢ä¸º DataFrame
        df = pd.DataFrame(values, columns=['timestamp', 'cpu_utilization'])
        
        # è½¬æ¢ç±»å‹
        df['timestamp'] = df['timestamp'].astype(int)
        df['cpu_utilization'] = df['cpu_utilization'].astype(float).round(2)
        
        print(f"âœ… æ•°æ®è·å–æˆåŠŸï¼Œå…± {len(df)} æ¡è®°å½•")
        return df
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {str(e)}")
        raise
    except (ValueError, KeyError) as e:
        print(f"âŒ æ•°æ®è§£æå¤±è´¥: {str(e)}")
        raise
    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯: {str(e)}")
        raise

def write_to_victoriametrics(
    vm_write_url: str,
    timestamp: int,
    cpu_value: float,
    status: int
) -> bool:
    """
    å°†CPUä½¿ç”¨ç‡å’ŒçŠ¶æ€å†™å…¥Victoriametrics
    
    Args:
        vm_write_url: Victoriametricså†™å…¥API URL
        timestamp: æ—¶é—´æˆ³ï¼ˆUnixæ—¶é—´æˆ³ï¼Œç§’ï¼‰
        cpu_value: CPUä½¿ç”¨ç‡
        status: çŠ¶æ€ï¼ˆ0è¡¨ç¤ºæ­£å¸¸ï¼Œ1è¡¨ç¤ºå¼‚å¸¸ï¼‰
        
    Returns:
        å†™å…¥æ˜¯å¦æˆåŠŸ
    """
    # æ„å»ºPrometheusæ ¼å¼çš„æ•°æ®
    # ç¡®ä¿æ—¶é—´æˆ³æ˜¯æ¯«ç§’çº§ï¼ˆVictoriametricsè¦æ±‚ï¼‰
    timestamp_ms = timestamp * 1000
    
    # æ„å»ºæŒ‡æ ‡æ•°æ®ï¼Œç¡®ä¿æ ‡ç­¾æ ¼å¼æ­£ç¡®
    metric_data = f'cpu_usage{{status="{status}"}} {cpu_value} {timestamp_ms}\n'
    
    print(f"ğŸ“¤ æ­£åœ¨å†™å…¥æ•°æ®åˆ°Victoriametrics...")
    print(f"   URL: {vm_write_url}")
    print(f"   æ•°æ®: {metric_data.strip()}")
    
    try:
        # å‘é€è¯·æ±‚
        response = requests.post(
            vm_write_url,
            data=metric_data,
            headers={'Content-Type': 'application/x-protobuf;proto=prometheus.WriteRequest'},
            verify=False  # å¿½ç•¥SSLè¯ä¹¦éªŒè¯
        )
        
        # æ£€æŸ¥å“åº”çŠ¶æ€
        response.raise_for_status()
        
        print(f"âœ… æ•°æ®å†™å…¥æˆåŠŸï¼")
        return True
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ å†™å…¥å¤±è´¥: {str(e)}")
        return False
    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯: {str(e)}")
        return False

# ======================
# ä¸»æ¨ç†å‡½æ•°
# ======================
def predict_anomalies(
    model_dir: str,
    output_csv_path: str = None,
    threshold: float = 0.5,
    vm_url: str = "https://1.1.1.1:30070/select/0/prometheus/api/v1/query_range",
    vm_token: str = "xxxxxxx",
    vm_namespace: str = "xxx-xxx",
    vm_host_name: str = "xxxxxxx",
    hours_to_fetch: float = 1,
    vm_write_url: str = "http://2.2.2.2:8428/api/v1/write"
):
    """
    æ¨ç†å‡½æ•°ï¼šåŠ è½½æ¨¡å‹ï¼Œé¢„æµ‹å¼‚å¸¸ï¼Œè¾“å‡ºç»“æœ

    Args:
        model_dir: æ¨¡å‹ç›®å½•ï¼ˆå« final_model.pkl, scaler, feature_namesï¼‰
        output_csv_path: è¾“å‡ºè·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸ä¿å­˜ç»“æœ
        threshold: è‡ªå®šä¹‰é˜ˆå€¼ï¼ˆé»˜è®¤0.5ï¼‰ï¼Œç”¨äºåˆ¤å®šå¼‚å¸¸çš„æ¦‚ç‡é˜ˆå€¼
        vm_url: Victoriametrics API URL
        vm_token: Victoriametricsè®¤è¯ä»¤ç‰Œ
        vm_namespace: å‘½åç©ºé—´
        vm_host_name: ä¸»æœºå
        hours_to_fetch: è·å–æœ€è¿‘å¤šå°‘å°æ—¶çš„æ•°æ®
        vm_write_url: Victoriametricså†™å…¥API URL
    """
    print(f"ğŸš€ å¼€å§‹æ¨ç†ä»»åŠ¡...")
    print(f"ğŸ“ æ¨¡å‹ç›®å½•: {model_dir}")
    if output_csv_path:
        print(f"ğŸ“ è¾“å‡ºè·¯å¾„: {output_csv_path}")
    
    print(f"ğŸ“¡ æ•°æ®æº: Victoriametrics")

    # 1. åŠ è½½æ¨¡å‹ç»„ä»¶
    model_path = os.path.join(model_dir, "cpu_anomaly_detector_with_unix_time.pkl")
    scaler_path = os.path.join(model_dir, "cpu_anomaly_detector_with_unix_time_scaler.pkl")
    feature_names_path = os.path.join(model_dir, "cpu_anomaly_detector_with_unix_time_feature_names.pkl")

    if not all(os.path.exists(p) for p in [model_path, scaler_path, feature_names_path]):
        raise FileNotFoundError("ç¼ºå°‘æ¨¡å‹æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ç›®å½•")

    model = joblib.load(model_path)
    scaler = joblib.load(scaler_path)
    feature_names = joblib.load(feature_names_path)

    print(f"âœ… æ¨¡å‹ç»„ä»¶åŠ è½½å®Œæˆ")
    print(f"   ç‰¹å¾æ•°é‡: {len(feature_names)}")

    # 2. è·å–æ•°æ®
    # ä»Victoriametricsè·å–æ•°æ®
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=hours_to_fetch)
    
    # æ ¼å¼åŒ–æ—¶é—´ä¸ºISOæ ¼å¼
    end_time_str = end_time.strftime("%Y-%m-%dT%H:%M:%SZ")
    start_time_str = start_time.strftime("%Y-%m-%dT%H:%M:%SZ")
    
    # è·å–æ•°æ®
    df = fetch_cpu_data_from_victoriametrics(
        vm_url=vm_url,
        query=None,  # ä½¿ç”¨é»˜è®¤æŸ¥è¯¢
        start_time=start_time_str,
        end_time=end_time_str,
        step="30",  # 30ç§’æ­¥é•¿
        token=vm_token,
        namespace=vm_namespace,
        host_name=vm_host_name
    )

    print(f"ğŸ“Š æµ‹è¯•æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"â„¹ï¸  å·²ä¿ç•™ CPU ä½¿ç”¨ç‡ 2 ä½å°æ•°")

    # 3. ç‰¹å¾å·¥ç¨‹
    engineer = CPUFeatureEngineer(window_sizes=[5, 15, 30, 60])
    feature_df, _ = engineer.create_all_features(df)

    # 4. æå–æ¨¡å‹æ‰€éœ€ç‰¹å¾
    X = feature_df[feature_names].values

    # 5. æ ‡å‡†åŒ–
    X_scaled = scaler.transform(X)

    # 6. é¢„æµ‹
    if hasattr(model, 'predict_proba'):
        # å¦‚æœæ¨¡å‹æ”¯æŒæ¦‚ç‡é¢„æµ‹
        y_proba = model.predict_proba(X_scaled)[:, 1]  # è·å–å¼‚å¸¸ç±»çš„æ¦‚ç‡
        y_pred = (y_proba >= threshold).astype(int)  # åº”ç”¨è‡ªå®šä¹‰é˜ˆå€¼
        print(f"â„¹ï¸  ä½¿ç”¨è‡ªå®šä¹‰é˜ˆå€¼ {threshold} è¿›è¡Œå¼‚å¸¸åˆ¤å®š")
        print(f"â„¹ï¸  é¢„æµ‹æ¦‚ç‡èŒƒå›´: min={y_proba.min():.2f}, max={y_proba.max():.2f}")
        # éªŒè¯æ˜¯å¦æœ‰æ¦‚ç‡ä½äºé˜ˆå€¼å´è¢«æ ‡è®°ä¸ºå¼‚å¸¸çš„æƒ…å†µ
        invalid_mask = (y_proba < threshold) & (y_pred == 1)
        if invalid_mask.any():
            print(f"âš ï¸  å‘ç° {invalid_mask.sum()} æ¡æ•°æ®æ¦‚ç‡ä½äºé˜ˆå€¼ä½†è¢«æ ‡è®°ä¸ºå¼‚å¸¸ï¼")
    else:
        # ä¸æ”¯æŒæ¦‚ç‡é¢„æµ‹çš„æ¨¡å‹
        y_pred = model.predict(X_scaled)  # 0: æ­£å¸¸, 1: å¼‚å¸¸
        print(f"âš ï¸  æ¨¡å‹ä¸æ”¯æŒæ¦‚ç‡é¢„æµ‹ï¼Œä½¿ç”¨é»˜è®¤é¢„æµ‹ç»“æœ")

    # 7. æ„é€ è¾“å‡ºï¼ˆè½¬æ¢ä¸º Unix æ—¶é—´æˆ³ï¼‰
    output_df = df[['timestamp', 'cpu_utilization']].copy()
    # å°† timestamp è½¬æ¢ä¸º Unix æ—¶é—´æˆ³ï¼ˆæ•´æ•°ç§’ï¼‰
    if not pd.api.types.is_numeric_dtype(output_df['timestamp']):
        output_df['timestamp'] = pd.to_datetime(output_df['timestamp']).astype('int64') // 10**9
    output_df['is_anomaly'] = y_pred.astype(int)

    # âœ… å†æ¬¡ç¡®ä¿ CPU ä¿ç•™ 2 ä½å°æ•°
    output_df['cpu_utilization'] = output_df['cpu_utilization'].round(2)

    # 8. ä¿å­˜ç»“æœ
    if output_csv_path:
        os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)
        output_df.to_csv(output_csv_path, index=False, float_format='%.2f')
        print(f"âœ… æ¨ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {output_csv_path}")
    else:
        print(f"âœ… æ¨ç†å®Œæˆï¼")
    
    # è¾“å‡ºç»“æœç»Ÿè®¡
    print(f"ğŸ“Š æ€»æ•°æ®ç‚¹: {len(y_pred)}, å¼‚å¸¸ç‚¹: {y_pred.sum()} ({y_pred.mean():.2%})")
    
    # è¾“å‡ºæœ€æ–°æ•°æ®ç‚¹çš„é¢„æµ‹ç»“æœ
    latest_point = output_df.iloc[-1]
    latest_time = pd.to_datetime(latest_point['timestamp'], unit='s')
    latest_cpu = latest_point['cpu_utilization']
    latest_anomaly = "å¼‚å¸¸" if latest_point['is_anomaly'] == 1 else "æ­£å¸¸"
    
    print(f"\nğŸ“Œ æœ€æ–°æ•°æ®ç‚¹ ({latest_time}):")
    print(f"   CPU ä½¿ç”¨ç‡: {latest_cpu:.2f}%")
    print(f"   çŠ¶æ€: {latest_anomaly}")
    
    # å¦‚æœæ˜¯å¼‚å¸¸ï¼Œè®¡ç®—å¼‚å¸¸æŒç»­æ—¶é—´
    if latest_point['is_anomaly'] == 1:
        # æŸ¥æ‰¾æœ€è¿‘è¿ç»­å¼‚å¸¸çš„èµ·å§‹ç‚¹
        anomaly_start_idx = len(output_df) - 1
        while anomaly_start_idx > 0 and output_df.iloc[anomaly_start_idx-1]['is_anomaly'] == 1:
            anomaly_start_idx -= 1
        
        anomaly_start_time = pd.to_datetime(output_df.iloc[anomaly_start_idx]['timestamp'], unit='s')
        duration_seconds = (latest_time - anomaly_start_time).total_seconds()
        duration_minutes = duration_seconds / 60
        
        print(f"   å¼‚å¸¸æŒç»­æ—¶é—´: {duration_minutes:.1f}åˆ†é’Ÿ")
    
    # å°†æœ€æ–°æ•°æ®ç‚¹å†™å…¥Victoriametrics
    if vm_write_url:
        print("\nğŸ“¤ æ­£åœ¨å°†æœ€æ–°æ•°æ®ç‚¹å†™å…¥Victoriametrics...")
        # è·å–æœ€æ–°æ•°æ®ç‚¹çš„æ—¶é—´æˆ³ã€CPUä½¿ç”¨ç‡å’ŒçŠ¶æ€
        latest_timestamp = int(latest_point['timestamp'])
        latest_cpu_value = float(latest_point['cpu_utilization'])
        latest_status = int(latest_point['is_anomaly'])
        
        # å†™å…¥Victoriametrics
        write_success = write_to_victoriametrics(
            vm_write_url=vm_write_url,
            timestamp=latest_timestamp,
            cpu_value=latest_cpu_value,
            status=latest_status
        )
        
        if write_success:
            print(f"âœ… æœ€æ–°æ•°æ®ç‚¹å·²æˆåŠŸå†™å…¥Victoriametrics")
        else:
            print(f"âŒ æœ€æ–°æ•°æ®ç‚¹å†™å…¥Victoriametricså¤±è´¥")
    
    return output_df

if __name__ == "__main__":
    # âœ… é…ç½®è·¯å¾„å’Œå‚æ•°
    MODEL_DIR = "../models/"                    # æ¨¡å‹æ–‡ä»¶å¤¹
    OUTPUT_CSV = "None"      # è¾“å‡ºç»“æœï¼Œè®¾ä¸ºNoneåˆ™ä¸ä¿å­˜
    THRESHOLD = 0.75                            # è‡ªå®šä¹‰é˜ˆå€¼
    
    # Victoriametricsé…ç½®
    VM_URL = "https://1.1.1.1:30070/select/0/prometheus/api/v1/query_range"
    VM_TOKEN = "xxxxxxx"  # å®é™…ä½¿ç”¨æ—¶è¯·æ›¿æ¢ä¸ºæœ‰æ•ˆçš„token
    VM_NAMESPACE = "xxx-Prod"
    VM_HOST_NAME = "xxx"
    HOURS_TO_FETCH = 1  # è·å–æœ€è¿‘1å°æ—¶çš„æ•°æ®
    
    # Victoriametricså†™å…¥é…ç½®
    VM_WRITE_URL = "http://2.2.2.2:8428/api/v1/import/prometheus"
    
    # æ·»åŠ å®šæ—¶å¾ªç¯
    import time
    while True:
        try:
            print(f"\nâ° å¼€å§‹æ–°ä¸€è½®æ¨ç†ä»»åŠ¡ ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})")
            # è¿è¡Œæ¨ç†
            result = predict_anomalies(
                model_dir=MODEL_DIR,
                output_csv_path=OUTPUT_CSV,
                threshold=THRESHOLD,
                vm_url=VM_URL,
                vm_token=VM_TOKEN,
                vm_namespace=VM_NAMESPACE,
                vm_host_name=VM_HOST_NAME,
                hours_to_fetch=HOURS_TO_FETCH,
                vm_write_url=VM_WRITE_URL
            )
            print(f"â³ ç­‰å¾…30ç§’åç»§ç»­...")
            time.sleep(30)
        except KeyboardInterrupt:
            print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ï¼Œåœæ­¢æ‰§è¡Œ")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
            print(f"â³ ç­‰å¾…30ç§’åé‡è¯•...")
            time.sleep(30)