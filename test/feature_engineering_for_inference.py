# inference_with_zenml_model.py
import os
import joblib
import pandas as pd
import numpy as np
from typing import Tuple, List
from datetime import datetime
import warnings

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore", category=FutureWarning, module="pandas")
warnings.filterwarnings("ignore", message="X does not have valid feature names")
warnings.filterwarnings("ignore", message="DataFrame.fillna with 'method' is deprecated")

# ======================
# ç‰¹å¾å·¥ç¨‹ç±»ï¼ˆä¸ ZenML pipeline ä¸€è‡´ï¼‰
# ======================
class CPUFeatureEngineer:
    """CPU ç‰¹å¾å·¥ç¨‹å¤„ç†å™¨ï¼Œç”¨äºå¼‚å¸¸æ£€æµ‹ï¼ˆä¸è®­ç»ƒ pipeline ä¿æŒä¸€è‡´ï¼‰"""

    def __init__(self, window_sizes: List[int] = None):
        self.window_sizes = window_sizes or [5, 15, 30, 60]  # åˆ†é’Ÿ
        self.feature_names = []

    def create_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºåŸºç¡€ç‰¹å¾"""
        df = df.copy()
        cpu_col = "cpu_utilization"

        # å½’ä¸€åŒ–
        df["cpu_normalized"] = (df[cpu_col] - df[cpu_col].min()) / (df[cpu_col].max() - df[cpu_col].min())

        # æ’åç™¾åˆ†ä½
        df["cpu_rank"] = df[cpu_col].rank(pct=True)

        # å…¨å±€åç¦»åº¦
        mean_cpu = df[cpu_col].mean()
        std_cpu = df[cpu_col].std()
        df["cpu_deviation_from_mean"] = (df[cpu_col] - mean_cpu) / (std_cpu + 1e-8)

        # æŒ‰å°æ—¶ã€æŒ‰æ—¥åŸºçº¿åç¦»
        df["hour"] = df["timestamp"].dt.hour
        df["day_of_week"] = df["timestamp"].dt.dayofweek
        df["date"] = df["timestamp"].dt.date

        hourly_mean = df.groupby("hour")[cpu_col].transform("mean")
        daily_mean = df.groupby("date")[cpu_col].transform("mean")
        df["cpu_deviation_hourly"] = df[cpu_col] - hourly_mean
        df["cpu_deviation_daily"] = df[cpu_col] - daily_mean

        # é˜ˆå€¼æ ‡è®°
        df["is_high_usage"] = (df[cpu_col] > 80).astype(int)
        df["is_very_high_usage"] = (df[cpu_col] > 90).astype(int)
        df["is_low_usage"] = (df[cpu_col] < 10).astype(int)

        self.feature_names.extend([
            "cpu_normalized", "cpu_rank", "cpu_deviation_from_mean",
            "cpu_deviation_hourly", "cpu_deviation_daily",
            "is_high_usage", "is_very_high_usage", "is_low_usage"
        ])
        return df

    def create_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºæ—¶é—´åºåˆ—ç‰¹å¾"""
        df = df.copy()
        cpu_col = "cpu_utilization"

        for window in self.window_sizes:
            win = f"{window}min"
            # å°† window è½¬æ¢ä¸ºåŸºäº 60 ç§’çš„æ•´æ•°ï¼ˆå¦‚ 5min = 5*60 = 300 è¡Œï¼‰
            rolled = df[cpu_col].rolling(window=window * 60, min_periods=1)

            df[f"{win}_mean"] = rolled.mean()
            df[f"{win}_std"] = rolled.std().fillna(0)
            df[f"{win}_max"] = rolled.max()
            df[f"{win}_min"] = rolled.min()
            df[f"{win}_range"] = df[f"{win}_max"] - df[f"{win}_min"]
            df[f"{win}_change"] = df[cpu_col] - df[f"{win}_mean"]
            df[f"{win}_pct_change"] = df[cpu_col] / (df[f"{win}_mean"] + 1e-8) - 1

            # è¶‹åŠ¿ï¼šçº¿æ€§å›å½’æ–œç‡è¿‘ä¼¼
            try:
                df[f"{win}_trend"] = rolled.apply(
                    lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) >= 2 else 0,
                    raw=True
                )
            except:
                df[f"{win}_trend"] = 0

            self.feature_names.extend([
                f"{win}_mean", f"{win}_std", f"{win}_max", f"{win}_min",
                f"{win}_range", f"{win}_change", f"{win}_pct_change", f"{win}_trend"
            ])

        # æ—¶é—´ä¸Šä¸‹æ–‡
        df["hour"] = df["timestamp"].dt.hour
        df["weekday"] = df["timestamp"].dt.dayofweek
        df["minute"] = df["timestamp"].dt.minute
        df["is_weekend"] = (df["weekday"] >= 5).astype(int)
        df["is_business_hour"] = ((df["hour"] >= 9) & (df["hour"] <= 17)).astype(int)
        df["is_peak_hour"] = ((df["hour"] >= 10) & (df["hour"] <= 16)).astype(int)

        self.feature_names.extend(["hour", "weekday", "minute", "is_weekend", "is_business_hour", "is_peak_hour"])
        return df

    def create_algorithm_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºåŸºäºç®—æ³•çš„å¼‚å¸¸æ£€æµ‹ç‰¹å¾"""
        from sklearn.ensemble import IsolationForest
        from sklearn.neighbors import LocalOutlierFactor

        df = df.copy()
        cpu_col = "cpu_utilization"

        # Isolation Forest
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        df["iso_forest_anomaly"] = iso_forest.fit_predict(df[[cpu_col]])
        df["iso_forest_anomaly"] = (df["iso_forest_anomaly"] == -1).astype(int)

        # LOF
        lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)
        df["lof_anomaly"] = lof.fit_predict(df[[cpu_col]])
        df["lof_anomaly"] = (df["lof_anomaly"] == -1).astype(int)

        # Z-score
        df["z_score"] = (df[cpu_col] - df[cpu_col].mean()) / (df[cpu_col].std() + 1e-8)
        df["z_score_anomaly"] = (df["z_score"].abs() > 3).astype(int)

        # IQR
        Q1 = df[cpu_col].quantile(0.25)
        Q3 = df[cpu_col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        df["iqr_anomaly"] = ((df[cpu_col] < lower) | (df[cpu_col] > upper)).astype(int)

        # ç®—æ³•å…±è¯†
        df["algorithm_consensus"] = df[["iso_forest_anomaly", "lof_anomaly", "z_score_anomaly", "iqr_anomaly"]].sum(axis=1)
        df["algorithm_agreement"] = (df["algorithm_consensus"] >= 2).astype(int)

        self.feature_names.extend([
            "iso_forest_anomaly", "lof_anomaly", "z_score", "z_score_anomaly",
            "iqr_anomaly", "algorithm_consensus", "algorithm_agreement"
        ])
        return df

    def create_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºäº¤äº’ä¸å¤åˆç‰¹å¾"""
        df = df.copy()

        # ç®—æ³•ä¸ç»Ÿè®¡äº¤äº’
        df["iso_x_deviation"] = df["iso_forest_anomaly"] * df["cpu_deviation_from_mean"]
        df["lof_x_zscore"] = df["lof_anomaly"] * df["z_score"]

        # æŒç»­æ€§ç‰¹å¾
        for n in [3, 5, 10]:
            df[f"high_usage_streak_{n}"] = (
                df["is_high_usage"].rolling(n, min_periods=1).sum()
            )
            df[f"anomaly_streak_{n}"] = (
                df["algorithm_agreement"].rolling(n, min_periods=1).sum()
            )

        # æ³¢åŠ¨æ€§
        for window in [15, 30, 60]:
            win = f"{window}min"
            df[f"{win}_cv"] = df[f"{win}_std"] / (df[f"{win}_mean"] + 1e-8)  # å˜å¼‚ç³»æ•°

        self.feature_names.extend([
            "iso_x_deviation", "lof_x_zscore",
            "high_usage_streak_3", "high_usage_streak_5", "high_usage_streak_10",
            "anomaly_streak_3", "anomaly_streak_5", "anomaly_streak_10",
            "15min_cv", "30min_cv", "60min_cv"
        ])
        return df

    def create_all_features(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
        """ä¸»æµç¨‹ï¼šä¾æ¬¡æ„å»ºæ‰€æœ‰ç‰¹å¾"""
        start_time = 0  # ä¸è®°å½•æ—¥å¿—

        # ç¡®ä¿ timestamp æ˜¯ datetime ç±»å‹
        df["timestamp"] = pd.to_datetime(df["timestamp"], unit='s')  # âœ… å…³é”®ï¼šUnix æ—¶é—´æˆ³è½¬ datetime
        df = df.sort_values("timestamp").reset_index(drop=True)

        print(f"ğŸ“Š å¼€å§‹ç‰¹å¾å·¥ç¨‹ï¼ŒåŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")

        df = self.create_basic_features(df)
        df = self.create_temporal_features(df)
        df = self.create_algorithm_features(df)
        df = self.create_interaction_features(df)

        # å¤„ç†ç¼ºå¤±å€¼
        df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)

        total_time = 0
        print(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œæœ€ç»ˆæ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"ğŸ§© ç”Ÿæˆç‰¹å¾æ•°é‡: {len(self.feature_names)}")

        return df, self.feature_names


# ======================
# ä¸»æ¨ç†å‡½æ•°
# ======================
def predict_anomalies(
    test_csv_path: str,
    model_dir: str,
    output_csv_path: str
):
    """
    æ¨ç†å‡½æ•°ï¼šåŠ è½½æ¨¡å‹ï¼Œé¢„æµ‹å¼‚å¸¸ï¼Œè¾“å‡ºç»“æœ

    Args:
        test_csv_path: æµ‹è¯•é›†è·¯å¾„ï¼ˆCSVï¼Œå« timestamp, cpu_utilizationï¼‰
        model_dir: æ¨¡å‹ç›®å½•ï¼ˆå« final_model.pkl, scaler, feature_namesï¼‰
        output_csv_path: è¾“å‡ºè·¯å¾„
    """
    print(f"ğŸš€ å¼€å§‹æ¨ç†ä»»åŠ¡...")
    print(f"ğŸ“ æµ‹è¯•é›†: {test_csv_path}")
    print(f"ğŸ“ æ¨¡å‹ç›®å½•: {model_dir}")
    print(f"ğŸ“ è¾“å‡ºè·¯å¾„: {output_csv_path}")

    # 1. åŠ è½½æ¨¡å‹ç»„ä»¶
    model_path = os.path.join(model_dir, "cpu_anomaly_detector_with_unix_time.pkl")
    scaler_path = os.path.join(model_dir, "cpu_anomaly_detector_with_unix_time_scaler.pkl")
    feature_names_path = os.path.join(model_dir, "cpu_anomaly_detector_with_unix_time_feature_names.pkl")

    if not all(os.path.exists(p) for p in [model_path, scaler_path, feature_names_path]):
        raise FileNotFoundError("ç¼ºå°‘æ¨¡å‹æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ç›®å½•")

    model = joblib.load(model_path)
    scaler = joblib.load(scaler_path)
    feature_names = joblib.load(feature_names_path)

    print(f"âœ… æ¨¡å‹ç»„ä»¶åŠ è½½å®Œæˆ")
    print(f"   ç‰¹å¾æ•°é‡: {len(feature_names)}")

    # 2. è¯»å–æµ‹è¯•æ•°æ®ï¼ˆåªå–å‰ä¸¤åˆ—ï¼‰
    df = pd.read_csv(
        test_csv_path,
        usecols=[0, 1],
        header=0,
        names=['timestamp', 'cpu_utilization']
    )

    # ç±»å‹è½¬æ¢
    df['timestamp'] = df['timestamp'].astype(int)
    df['cpu_utilization'] = df['cpu_utilization'].astype(float)

    # âœ… ä¿ç•™ 2 ä½å°æ•°ï¼ˆå››èˆäº”å…¥ï¼‰
    df['cpu_utilization'] = df['cpu_utilization'].round(2)

    print(f"ğŸ“Š æµ‹è¯•æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"â„¹ï¸  å·²ä¿ç•™ CPU ä½¿ç”¨ç‡ 2 ä½å°æ•°")

    # 3. ç‰¹å¾å·¥ç¨‹
    engineer = CPUFeatureEngineer(window_sizes=[5, 15, 30, 60])
    feature_df, _ = engineer.create_all_features(df)

    # 4. æå–æ¨¡å‹æ‰€éœ€ç‰¹å¾
    X = feature_df[feature_names].values

    # 5. æ ‡å‡†åŒ–
    X_scaled = scaler.transform(X)

    # 6. é¢„æµ‹
    y_pred = model.predict(X_scaled)  # 0: æ­£å¸¸, 1: å¼‚å¸¸

    # 7. æ„é€ è¾“å‡º
    output_df = df[['timestamp', 'cpu_utilization']].copy()
    output_df['is_anomaly'] = y_pred.astype(int)

    # âœ… å†æ¬¡ç¡®ä¿ CPU ä¿ç•™ 2 ä½å°æ•°
    output_df['cpu_utilization'] = output_df['cpu_utilization'].round(2)

    # 8. ä¿å­˜ç»“æœ
    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)
    output_df.to_csv(output_csv_path, index=False, float_format='%.1f')
    print(f"âœ… æ¨ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {output_csv_path}")
    print(f"ğŸ“Š æ€»æ•°æ®ç‚¹: {len(y_pred)}, å¼‚å¸¸ç‚¹: {y_pred.sum()} ({y_pred.mean():.2%})")

    return output_df


# ======================
# ä½¿ç”¨ç¤ºä¾‹
# ======================
if __name__ == "__main__":
    # âœ… é…ç½®è·¯å¾„
    TEST_CSV = "../data/cpu_data_timestamp.csv"           # è¾“å…¥æµ‹è¯•é›†
    MODEL_DIR = "../models/"                    # æ¨¡å‹æ–‡ä»¶å¤¹
    OUTPUT_CSV = "../output/anomalies.csv"      # è¾“å‡ºç»“æœ

    # è¿è¡Œæ¨ç†
    result = predict_anomalies(TEST_CSV, MODEL_DIR, OUTPUT_CSV)