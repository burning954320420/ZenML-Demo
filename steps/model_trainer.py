# steps/model_trainer.py
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight
import warnings
from zenml import step
from zenml.logger import get_logger

logger = get_logger(__name__)
warnings.filterwarnings('ignore')

class CPURandomForestTrainer:
    """CPUå¼‚å¸¸æ£€æµ‹Random Forestè®­ç»ƒå™¨"""
    def __init__(self, test_size=0.2, random_state=42):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        Args:
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            random_state: éšæœºç§å­
        """
        self.test_size = test_size
        self.random_state = random_state
        self.model = None
        self.scaler = None
        self.feature_importance = None
        self.training_history = {}
        logger.info(f"ğŸ¯ Random Forestè®­ç»ƒå™¨åˆå§‹åŒ–")
        logger.info(f"   æµ‹è¯•é›†æ¯”ä¾‹: {test_size}")
        logger.info(f"   éšæœºç§å­: {random_state}")

    def prepare_training_data(self, feature_df, feature_list, target_col='is_anomaly'):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        logger.info("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
        # åˆ›å»ºç‰¹å¾çŸ©é˜µ
        X = feature_df[feature_list].copy()
        y = feature_df[target_col].copy()
        # æ£€æŸ¥æ•°æ®è´¨é‡
        logger.info(f"   æ•°æ®å½¢çŠ¶: {X.shape}")
        logger.info(f"   å¼‚å¸¸æ¯”ä¾‹: {y.mean():.3f}")
        logger.info(f"   ç¼ºå¤±å€¼: {X.isnull().sum().sum()}")
        # å¤„ç†æ— ç©·å€¼
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(0)
        # åˆ†å‰²è®­ç»ƒæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=self.test_size,
            random_state=self.random_state,
            stratify=y
        )
        # æ ‡å‡†åŒ–ç‰¹å¾
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        logger.info(f"   è®­ç»ƒé›†: {X_train.shape}, å¼‚å¸¸æ¯”ä¾‹: {y_train.mean():.3f}")
        logger.info(f"   æµ‹è¯•é›†: {X_test.shape}, å¼‚å¸¸æ¯”ä¾‹: {y_test.mean():.3f}")
        self.training_history.update({
            'feature_names': feature_list,
            'train_size': len(X_train),
            'test_size': len(X_test),
            'anomaly_ratio': y.mean()
        })
        return X_train_scaled, X_test_scaled, y_train, y_test, X_train, X_test

    def hyperparameter_tuning(self, X_train, y_train):
        """è¶…å‚æ•°è°ƒä¼˜"""
        logger.info("ğŸ” å¼€å§‹è¶…å‚æ•°è°ƒä¼˜...")
        # è®¡ç®—ç±»åˆ«æƒé‡
        class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
        class_weight_dict = {0: class_weights[0], 1: class_weights[1]}
        logger.info(f"   ç±»åˆ«æƒé‡: {class_weight_dict}")
        # å‚æ•°ç½‘æ ¼
        param_grid = {
            'n_estimators': [100, 200],
            'max_depth': [10, 15, None],
            'min_samples_split': [5, 10],
            'min_samples_leaf': [2, 5],
            'max_features': ['auto', 'sqrt']
        }
        # åŸºç¡€æ¨¡å‹
        rf_base = RandomForestClassifier(
            random_state=self.random_state,
            class_weight=class_weight_dict,
            n_jobs=-1
        )
        # ç½‘æ ¼æœç´¢
        grid_search = GridSearchCV(
            rf_base, param_grid,
            cv=3, scoring='f1',
            n_jobs=-1, verbose=1
        )
        logger.info("   æ‰§è¡Œç½‘æ ¼æœç´¢...")
        grid_search.fit(X_train, y_train)
        # æœ€ä½³å‚æ•°
        best_params = grid_search.best_params_
        best_score = grid_search.best_score_
        logger.info(f"   âœ… æœ€ä½³F1åˆ†æ•°: {best_score:.4f}")
        logger.info(f"   âœ… æœ€ä½³å‚æ•°: {best_params}")
        self.training_history.update({
            'best_params': best_params,
            'best_cv_score': best_score,
            'class_weights': class_weight_dict
        })
        return grid_search.best_estimator_

    def train_model(self, X_train, y_train):
        """è®­ç»ƒæœ€ç»ˆæ¨¡å‹"""
        logger.info("ğŸš€ è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
        # è¶…å‚æ•°è°ƒä¼˜
        self.model = self.hyperparameter_tuning(X_train, y_train)
        # äº¤å‰éªŒè¯è¯„ä¼°
        cv_scores = cross_val_score(self.model, X_train, y_train, cv=3, scoring='f1')
        logger.info(f"   äº¤å‰éªŒè¯F1åˆ†æ•°: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
        # é‡æ–°è®­ç»ƒå®Œæ•´æ¨¡å‹
        self.model.fit(X_train, y_train)
        # ç‰¹å¾é‡è¦æ€§
        self.feature_importance = pd.DataFrame({
            'feature': self.training_history['feature_names'],
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        logger.info(f"   âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        logger.info(f"   ğŸ“Š ç‰¹å¾é‡è¦æ€§å‰5: ")
        for idx, row in self.feature_importance.head().iterrows():
            logger.info(f"      {row['feature']}: {row['importance']:.4f}")
        self.training_history.update({
            'cv_f1_mean': cv_scores.mean(),
            'cv_f1_std': cv_scores.std(),
            'feature_importance': self.feature_importance.to_dict('records')
        })
        return self.model

    def evaluate_model(self, X_test, y_test):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        logger.info("ğŸ“ˆ è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        # é¢„æµ‹
        y_pred = self.model.predict(X_test)
        y_pred_proba = self.model.predict_proba(X_test)[:, 1]
        # åˆ†ç±»æŠ¥å‘Š
        logger.info("\nğŸ“‹ åˆ†ç±»æŠ¥å‘Š:")
        logger.info(classification_report(y_test, y_pred))
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_test, y_pred)
        logger.info(f"\nğŸ”„ æ··æ·†çŸ©é˜µ:")
        logger.info(f"çœŸè´Ÿä¾‹: {cm[0,0]}, å‡æ­£ä¾‹: {cm[0,1]}")
        logger.info(f"å‡è´Ÿä¾‹: {cm[1,0]}, çœŸæ­£ä¾‹: {cm[1,1]}")
        # ROCå’ŒPRæ›²çº¿
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        roc_auc = auc(fpr, tpr)
        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
        pr_auc = auc(recall, precision)
        logger.info(f"\nğŸ¯ å…³é”®æŒ‡æ ‡:")
        logger.info(f"   ROC AUC: {roc_auc:.4f}")
        logger.info(f"   PR AUC: {pr_auc:.4f}")
        # ä¿å­˜è¯„ä¼°ç»“æœ
        self.training_history.update({
            'test_roc_auc': roc_auc,
            'test_pr_auc': pr_auc,
            'confusion_matrix': cm.tolist(),
        })
        return {
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba,
            'roc_auc': roc_auc,
            'pr_auc': pr_auc,
            'confusion_matrix': cm
        }

from typing import Tuple, Dict, List, Any

@step
def train_model_step(
    feature_df: pd.DataFrame,
    feature_list: list,
    target_col: str = 'is_anomaly',
    test_size: float = 0.2,
    random_state: int = 42
) -> Tuple[RandomForestClassifier, StandardScaler, List[str], Dict[str, Any]]:
    """
    è®­ç»ƒCPUå¼‚å¸¸æ£€æµ‹éšæœºæ£®æ—æ¨¡å‹

    Args:
        feature_df: åŒ…å«ç‰¹å¾çš„DataFrame
        feature_list: ç‰¹å¾åˆ—è¡¨
        target_col: ç›®æ ‡åˆ—å
        test_size: æµ‹è¯•é›†æ¯”ä¾‹
        random_state: éšæœºç§å­

    Returns:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        scaler: ç‰¹å¾æ ‡å‡†åŒ–å™¨
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        training_history: è®­ç»ƒå†å²è®°å½•
    """
    logger.info("ğŸš€ å¼€å§‹CPUå¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒ...")

    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = CPURandomForestTrainer(test_size=test_size, random_state=random_state)

    # å‡†å¤‡è®­ç»ƒæ•°æ®
    X_train_scaled, X_test_scaled, y_train, y_test, _, _ = trainer.prepare_training_data(
        feature_df, feature_list, target_col=target_col
    )

    # è®­ç»ƒæ¨¡å‹
    model = trainer.train_model(X_train_scaled, y_train)

    # è¯„ä¼°æ¨¡å‹
    trainer.evaluate_model(X_test_scaled, y_test)

    logger.info("âœ… CPUå¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒå®Œæˆ!")

    # è¿”å›æ¨¡å‹ã€æ ‡å‡†åŒ–å™¨ã€ç‰¹å¾åç§°å’Œè®­ç»ƒå†å²
    return model, trainer.scaler, feature_list, trainer.training_history
