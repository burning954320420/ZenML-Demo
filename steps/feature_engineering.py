# steps/feature_engineering.py

import pandas as pd
import numpy as np
from zenml import step
from zenml.logger import get_logger
from typing import Tuple, List
import time
from typing import Optional, List, Tuple

logger = get_logger(__name__)

class CPUFeatureEngineer:
    """CPU ç‰¹å¾å·¥ç¨‹å¤„ç†å™¨ï¼Œç”¨äºå¼‚å¸¸æ£€æµ‹"""

    def __init__(self, window_sizes: List[int] = None):
        self.window_sizes = window_sizes or [5, 15, 30, 60]  # åˆ†é’Ÿ
        self.feature_names = []

    def create_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºåŸºç¡€ç‰¹å¾"""
        df = df.copy()
        cpu_col = "cpu_utilization"

        # å½’ä¸€åŒ–
        df["cpu_normalized"] = (df[cpu_col] - df[cpu_col].min()) / (df[cpu_col].max() - df[cpu_col].min())

        # æ’åç™¾åˆ†ä½
        df["cpu_rank"] = df[cpu_col].rank(pct=True)

        # å…¨å±€åç¦»åº¦
        mean_cpu = df[cpu_col].mean()
        std_cpu = df[cpu_col].std()
        df["cpu_deviation_from_mean"] = (df[cpu_col] - mean_cpu) / (std_cpu + 1e-8)

        # æŒ‰å°æ—¶ã€æŒ‰æ—¥åŸºçº¿åç¦»
        df["hour"] = df["timestamp"].dt.hour
        df["day_of_week"] = df["timestamp"].dt.dayofweek
        df["date"] = df["timestamp"].dt.date

        hourly_mean = df.groupby("hour")[cpu_col].transform("mean")
        daily_mean = df.groupby("date")[cpu_col].transform("mean")
        df["cpu_deviation_hourly"] = df[cpu_col] - hourly_mean
        df["cpu_deviation_daily"] = df[cpu_col] - daily_mean

        # é˜ˆå€¼æ ‡è®°
        df["is_high_usage"] = (df[cpu_col] > 80).astype(int)
        df["is_very_high_usage"] = (df[cpu_col] > 90).astype(int)
        df["is_low_usage"] = (df[cpu_col] < 10).astype(int)

        self.feature_names.extend([
            "cpu_normalized", "cpu_rank", "cpu_deviation_from_mean",
            "cpu_deviation_hourly", "cpu_deviation_daily",
            "is_high_usage", "is_very_high_usage", "is_low_usage"
        ])
        return df

    def create_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºæ—¶é—´åºåˆ—ç‰¹å¾"""
        df = df.copy()
        cpu_col = "cpu_utilization"

        for window in self.window_sizes:
            win = f"{window}min"
            rolled = df[cpu_col].rolling(window, min_periods=1)

            df[f"{win}_mean"] = rolled.mean()
            df[f"{win}_std"] = rolled.std().fillna(0)
            df[f"{win}_max"] = rolled.max()
            df[f"{win}_min"] = rolled.min()
            df[f"{win}_range"] = df[f"{win}_max"] - df[f"{win}_min"]
            df[f"{win}_change"] = df[cpu_col] - df[f"{win}_mean"]
            df[f"{win}_pct_change"] = df[cpu_col] / (df[f"{win}_mean"] + 1e-8) - 1

            # è¶‹åŠ¿ï¼šçº¿æ€§å›å½’æ–œç‡è¿‘ä¼¼
            try:
                df[f"{win}_trend"] = rolled.apply(
                    lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) >= 2 else 0, 
                    raw=True
                )
            except:
                df[f"{win}_trend"] = 0

            self.feature_names.extend([
                f"{win}_mean", f"{win}_std", f"{win}_max", f"{win}_min",
                f"{win}_range", f"{win}_change", f"{win}_pct_change", f"{win}_trend"
            ])

        # æ—¶é—´ä¸Šä¸‹æ–‡
        df["hour"] = df["timestamp"].dt.hour
        df["weekday"] = df["timestamp"].dt.dayofweek
        df["minute"] = df["timestamp"].dt.minute
        df["is_weekend"] = (df["weekday"] >= 5).astype(int)
        df["is_business_hour"] = ((df["hour"] >= 9) & (df["hour"] <= 17)).astype(int)
        df["is_peak_hour"] = ((df["hour"] >= 10) & (df["hour"] <= 16)).astype(int)

        self.feature_names.extend(["hour", "weekday", "minute", "is_weekend", "is_business_hour", "is_peak_hour"])
        return df

    def create_algorithm_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºåŸºäºç®—æ³•çš„å¼‚å¸¸æ£€æµ‹ç‰¹å¾"""
        from sklearn.ensemble import IsolationForest
        from sklearn.neighbors import LocalOutlierFactor

        df = df.copy()
        cpu_col = "cpu_utilization"

        # Isolation Forest
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        df["iso_forest_anomaly"] = iso_forest.fit_predict(df[[cpu_col]])
        df["iso_forest_anomaly"] = (df["iso_forest_anomaly"] == -1).astype(int)

        # LOF
        lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)
        df["lof_anomaly"] = lof.fit_predict(df[[cpu_col]])
        df["lof_anomaly"] = (df["lof_anomaly"] == -1).astype(int)

        # Z-score
        df["z_score"] = (df[cpu_col] - df[cpu_col].mean()) / (df[cpu_col].std() + 1e-8)
        df["z_score_anomaly"] = (df["z_score"].abs() > 3).astype(int)

        # IQR
        Q1 = df[cpu_col].quantile(0.25)
        Q3 = df[cpu_col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        df["iqr_anomaly"] = ((df[cpu_col] < lower) | (df[cpu_col] > upper)).astype(int)

        # ç®—æ³•å…±è¯†
        df["algorithm_consensus"] = df[["iso_forest_anomaly", "lof_anomaly", "z_score_anomaly", "iqr_anomaly"]].sum(axis=1)
        df["algorithm_agreement"] = (df["algorithm_consensus"] >= 2).astype(int)

        self.feature_names.extend([
            "iso_forest_anomaly", "lof_anomaly", "z_score", "z_score_anomaly",
            "iqr_anomaly", "algorithm_consensus", "algorithm_agreement"
        ])
        return df

    def create_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºäº¤äº’ä¸å¤åˆç‰¹å¾"""
        df = df.copy()

        # ç®—æ³•ä¸ç»Ÿè®¡äº¤äº’
        df["iso_x_deviation"] = df["iso_forest_anomaly"] * df["cpu_deviation_from_mean"]
        df["lof_x_zscore"] = df["lof_anomaly"] * df["z_score"]

        # æŒç»­æ€§ç‰¹å¾
        for n in [3, 5, 10]:
            df[f"high_usage_streak_{n}"] = (
                df["is_high_usage"].rolling(n, min_periods=1).sum()
            )
            df[f"anomaly_streak_{n}"] = (
                df["algorithm_agreement"].rolling(n, min_periods=1).sum()
            )

        # æ³¢åŠ¨æ€§
        for window in [15, 30, 60]:
            win = f"{window}min"
            df[f"{win}_cv"] = df[f"{win}_std"] / (df[f"{win}_mean"] + 1e-8)  # å˜å¼‚ç³»æ•°

        self.feature_names.extend([
            "iso_x_deviation", "lof_x_zscore",
            "high_usage_streak_3", "high_usage_streak_5", "high_usage_streak_10",
            "anomaly_streak_3", "anomaly_streak_5", "anomaly_streak_10",
            "15min_cv", "30min_cv", "60min_cv"
        ])
        return df

    def create_all_features(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
        """ä¸»æµç¨‹ï¼šä¾æ¬¡æ„å»ºæ‰€æœ‰ç‰¹å¾"""
        start_time = time.time()

        # ç¡®ä¿ timestamp æ˜¯ datetime ç±»å‹
        df["timestamp"] = pd.to_datetime(df["timestamp"])
        df = df.sort_values("timestamp").reset_index(drop=True)

        print(f"ğŸ“Š å¼€å§‹ç‰¹å¾å·¥ç¨‹ï¼ŒåŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")

        df = self.create_basic_features(df)
        df = self.create_temporal_features(df)
        df = self.create_algorithm_features(df)
        df = self.create_interaction_features(df)

        # å¤„ç†ç¼ºå¤±å€¼
        df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)

        total_time = time.time() - start_time
        logger.info(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œè€—æ—¶: {total_time:.2f} ç§’")
        logger.info(f"ğŸ“Š æœ€ç»ˆæ•°æ®å½¢çŠ¶: {df.shape}")
        logger.info(f"ğŸ§© ç”Ÿæˆç‰¹å¾æ•°é‡: {len(self.feature_names)}")

        return df, self.feature_names


@step
def feature_engineering_step(
    raw_df: pd.DataFrame,
    window_sizes: Optional[List[int]] = None
) -> Tuple[pd.DataFrame, List[str]]:
    """
    ZenML Stepï¼šæ‰§è¡Œ CPU ç‰¹å¾å·¥ç¨‹

    Args:
        raw_df: è¾“å…¥çš„åŸå§‹æ•°æ®ï¼ŒåŒ…å« timestamp, cpu_utilization, is_anomaly
        window_sizes: æ»‘åŠ¨çª—å£å¤§å°ï¼ˆåˆ†é’Ÿï¼‰ï¼Œé»˜è®¤ [5, 15, 30, 60]

    Returns:
        Tuple[å¢å¼ºåçš„ DataFrame, ç‰¹å¾åç§°åˆ—è¡¨]
    """
    logger.info("ğŸ”§ å¼€å§‹æ‰§è¡Œç‰¹å¾å·¥ç¨‹ Step...")

    if raw_df.empty:
        raise ValueError("è¾“å…¥æ•°æ®ä¸ºç©º")

    required_columns = {"timestamp", "cpu_utilization", "is_anomaly"}
    missing = required_columns - set(raw_df.columns)
    if missing:
        raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—: {missing}")

    # åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å¸ˆ
    engineer = CPUFeatureEngineer(window_sizes=window_sizes)

    # æ‰§è¡Œç‰¹å¾å·¥ç¨‹
    try:
        processed_df, feature_names = engineer.create_all_features(raw_df)
        logger.info(f"âœ… ç‰¹å¾å·¥ç¨‹æˆåŠŸï¼Œç”Ÿæˆ {len(feature_names)} ä¸ªç‰¹å¾")
        return processed_df, feature_names
    except Exception as e:
        logger.error(f"âŒ ç‰¹å¾å·¥ç¨‹å¤±è´¥: {e}")
        raise