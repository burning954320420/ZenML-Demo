# steps/data_validator.py
import pandas as pd
from zenml import step
from zenml.logger import get_logger
from typing import Dict, Any

logger = get_logger(__name__)

@step
def data_validation_step(df: pd.DataFrame) -> Dict[str, float]:
    """
    ZenML Step: éªŒè¯ CPU æ•°æ®è´¨é‡ï¼ˆè‡ªåŠ¨å¤„ç†æ—¶é—´ç´¢å¼•ï¼‰

    æ”¯æŒè¾“å…¥ï¼š
      - ç´¢å¼•ä¸º DatetimeIndex âœ…
      - åŒ…å« 'datetime' åˆ—ï¼ˆè‡ªåŠ¨è½¬ä¸ºç´¢å¼•ï¼‰
      - åŒ…å« 'timestamp' åˆ—ï¼ˆè‡ªåŠ¨è½¬ä¸º datetime å¹¶è®¾ä¸ºç´¢å¼•ï¼‰

    Args:
        df: è¾“å…¥ DataFrame

    Returns:
        éªŒè¯æŒ‡æ ‡å­—å…¸
    """
    logger.info("ğŸ” å¼€å§‹æ•°æ®è´¨é‡éªŒè¯...")

    # ========================
    # 1. è‡ªåŠ¨å¤„ç†æ—¶é—´ç´¢å¼•ï¼ˆå…³é”®ä¿®å¤ï¼‰
    # ========================
    if not isinstance(df.index, pd.DatetimeIndex):
        logger.warning("âš ï¸ ç´¢å¼•ä¸æ˜¯ DatetimeIndexï¼Œå°è¯•ä»åˆ—æ„å»ºæ—¶é—´ç´¢å¼•...")

        # æƒ…å†µ1: å­˜åœ¨ 'datetime' åˆ—ï¼ˆå­—ç¬¦ä¸²æˆ–æ—¶é—´ç±»å‹ï¼‰
        if 'datetime' in df.columns:
            df['datetime'] = pd.to_datetime(df['datetime'])
            df = df.set_index('datetime').sort_index()
            logger.info("âœ… ä½¿ç”¨ 'datetime' åˆ—æ„å»ºæ—¶é—´ç´¢å¼•")

        # æƒ…å†µ2: å­˜åœ¨ 'timestamp' åˆ—ï¼ˆUnix æ—¶é—´æˆ³ï¼‰
        elif 'timestamp' in df.columns:
            df['timestamp'] = pd.to_numeric(df['timestamp'], errors='coerce')
            # åˆ¤æ–­æ˜¯ç§’è¿˜æ˜¯æ¯«ç§’
            is_millisecond = (df['timestamp'].max() > 1e10)
            unit = 'ms' if is_millisecond else 's'
            df['datetime'] = pd.to_datetime(df['timestamp'], unit=unit)
            df = df.set_index('datetime').sort_index()
            logger.info(f"âœ… ä½¿ç”¨ 'timestamp' åˆ—ï¼ˆunit={unit}ï¼‰æ„å»ºæ—¶é—´ç´¢å¼•")

        else:
            raise ValueError(
                "âŒ æ— æ³•æ„å»ºæ—¶é—´ç´¢å¼•ï¼šDataFrame å¿…é¡»åŒ…å« 'datetime' æˆ– 'timestamp' åˆ—ï¼Œ"
                "æˆ–å…¶ç´¢å¼•å¿…é¡»æ˜¯ DatetimeIndex"
            )
    else:
        logger.info("âœ… è¾“å…¥ç´¢å¼•å·²æ˜¯ DatetimeIndexï¼Œè·³è¿‡è½¬æ¢")

    # ========================
    # 2. æ£€æŸ¥å¿…éœ€åˆ—
    # ========================
    cpu_col = 'cpu_utilization'
    required_columns = [cpu_col, 'is_anomaly']
    missing_cols = [col for col in required_columns if col not in df.columns]

    if missing_cols:
        raise ValueError(f"âŒ ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")

    print("ğŸ” Performing data quality validation...")
    print("=" * 50)

    # ========================
    # 3. åŸºç¡€è´¨é‡æ£€æŸ¥
    # ========================
    print("ğŸ“Š Basic quality metrics:")

    # å®Œæ•´æ€§
    completeness = (1 - df.isnull().sum().sum() / len(df)) * 100
    print(f"   Data completeness: {completeness:.2f}%")

    # æ—¶é—´è¿ç»­æ€§ï¼ˆå•è°ƒé€’å¢ï¼‰
    is_continuous = df.index.is_monotonic_increasing
    print(f"   Time continuity: {'âœ… Good' if is_continuous else 'âŒ Issues'}")

    # å€¼æœ‰æ•ˆæ€§ï¼ˆCPU åº”åœ¨ 0~100ï¼‰
    invalid_values = ((df[cpu_col] < 0) | (df[cpu_col] > 100)).sum()
    validity_ratio = (len(df) - invalid_values) / len(df) * 100
    validity_status = 'âœ…' if invalid_values == 0 else 'âŒ'
    print(f"   Value validity: {len(df) - invalid_values}/{len(df)} ({validity_status})")

    # å¼‚å¸¸æ¯”ä¾‹
    anomaly_ratio = df['is_anomaly'].mean()
    anomaly_status = 'âœ…' if 0.05 <= anomaly_ratio <= 0.15 else 'âš ï¸'
    print(f"   Anomaly ratio: {anomaly_ratio:.2%} ({anomaly_status})")

    # ========================
    # 4. æ¨¡å¼åˆç†æ€§æ£€æŸ¥
    # ========================
    print(f"\nğŸ“ˆ Pattern validation:")

    # å·¥ä½œæ—¥ vs å‘¨æœ«
    weekday_mean = df[df.index.dayofweek < 5][cpu_col].mean()
    weekend_mean = df[df.index.dayofweek >= 5][cpu_col].mean()
    workday_difference = weekday_mean - weekend_mean
    workday_status = 'âœ…' if workday_difference > 5 else 'âš ï¸'
    print(f"   Weekday difference: {workday_difference:.1f}% ({workday_status})")

    # æ—¥é—´æ³¢åŠ¨ï¼ˆå°æ—¶çº§å‡å€¼æœ€å¤§æœ€å°å·®ï¼‰
    hourly_mean = df.groupby(df.index.hour)[cpu_col].mean()
    daily_variation = hourly_mean.max() - hourly_mean.min()
    daily_status = 'âœ…' if daily_variation > 20 else 'âš ï¸'
    print(f"   Daily variation: {daily_variation:.1f}% ({daily_status})")

    # å¼‚å¸¸åˆ†å¸ƒï¼ˆé˜²èšé›†ï¼‰
    anomaly_points = df[df['is_anomaly'] == 1]
    if len(anomaly_points) > 0:
        time_gaps = anomaly_points.index.to_series().diff().dt.total_seconds() / 60  # åˆ†é’Ÿ
        avg_gap = time_gaps.mean()
        gap_status = 'âœ…' if avg_gap > 60 else 'âš ï¸'
        print(f"   Anomaly distribution: avg gap {avg_gap:.0f} min ({gap_status})")
    else:
        avg_gap = float('nan')
        print("   Anomaly distribution: no anomalies found")

    # ========================
    # 5. ç»Ÿè®¡æ‘˜è¦
    # ========================
    print(f"\nğŸ“‹ Data statistics summary:")
    stats = df[cpu_col].describe()
    for stat_name, value in stats.items():
        print(f"   {stat_name}: {value:.2f}%")

    # ========================
    # 6. è¿”å›ç»“æ„åŒ–ç»“æœ
    # ========================
    results = {
        'completeness': float(completeness),
        'validity': float(validity_ratio),
        'anomaly_ratio': float(anomaly_ratio * 100),
        'workday_difference': float(workday_difference),
        'daily_variation': float(daily_variation),
        'avg_anomaly_gap_min': avg_gap if not pd.isna(avg_gap) else -1,
        'n_anomalies': int(anomaly_points.shape[0]),
        'n_total': int(df.shape[0]),
        'time_continuity': 1 if is_continuous else 0
    }

    logger.info("âœ… æ•°æ®è´¨é‡éªŒè¯å®Œæˆ")
    return results